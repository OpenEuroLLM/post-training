#!/bin/bash
# ============================================================================
# Auto-generated containerized TRL SLURM job script — do not edit by hand.
# Re-generate via: python scripts/submit.py --config <your_config.yaml>
# ============================================================================

#SBATCH --job-name={{ job_name }}
#SBATCH --partition={{ partition }}
#SBATCH --nodes={{ num_nodes }}
#SBATCH --ntasks-per-node=1
#SBATCH --gres=gpu:{{ gpus_per_node }}
{% if cpus_per_gpu is integer and cpus_per_gpu > 0 -%}
#SBATCH --cpus-per-gpu={{ cpus_per_gpu }}
{% endif -%}
#SBATCH --time={{ wall_time }}
#SBATCH --signal=B:USR1@{{ signal_time_seconds }}
#SBATCH --output={{ run_dir }}/slurm/slurm-%j.out
#SBATCH --error={{ run_dir }}/slurm/slurm-%j.err
#SBATCH --open-mode=append

set -euo pipefail

# ── Source cluster environment ────────────────────────────────────────────
{% if env_file %}source "{{ env_file }}"{% endif %}

# ── Distributed setup ─────────────────────────────────────────────────────
MASTER_ADDR=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
MASTER_ADDR=$(getent hosts "$MASTER_ADDR" | awk '{print $1}')
export MASTER_ADDR
export MASTER_PORT=$((29500 + SLURM_JOB_ID % 2000))
export GPUS_PER_NODE={{ gpus_per_node }}
export WORLD_SIZE=$(( SLURM_NNODES * GPUS_PER_NODE ))

# NCCL tuning for multi-node stability
export NCCL_IB_TIMEOUT=120
export NCCL_DEBUG=INFO
export CUDA_DEVICE_MAX_CONNECTIONS=1
export OMP_NUM_THREADS=1

echo "=========================================="
echo "Job started at: $(date)"
echo "Job ID: $SLURM_JOB_ID"
echo "Nodes: $SLURM_NNODES x $GPUS_PER_NODE GPUs (WORLD_SIZE=$WORLD_SIZE)"
echo "MASTER_ADDR:MASTER_PORT = ${MASTER_ADDR}:${MASTER_PORT}"
echo "Config: {{ config_path }}"
echo "Container: {{ container_image }}"
echo "=========================================="

# ── Self-healing machinery ────────────────────────────────────────────────
FAILURE_COUNT_FILE="{{ run_dir }}/slurm/failure_count"
MAX_FAILURES={{ max_failures }}

if [ ! -f "$FAILURE_COUNT_FILE" ]; then
    echo 0 > "$FAILURE_COUNT_FILE"
fi

handle_signal() {
    echo "[$(date)] Received SIGUSR1 — timeout approaching. Requeuing..."
    scontrol requeue $SLURM_JOB_ID
}
trap handle_signal USR1

# ── Launch inside Singularity container ───────────────────────────────────
srun --export=ALL --wait=60 --kill-on-bad-exit=1 \
  singularity exec --nv \
{% for mount in bind_mounts %}  --bind "{{ mount }}" \
{% endfor %}  "{{ container_image }}" \
  bash -lc "
    set -e

    # Prevent host Python/PATH from interfering with container
    export PATH=\"/usr/local/bin:/usr/bin:/bin\"
    export PYTHONPATH=\"\"
    export PYTHONNOUSERSITE=1

    export NODE_RANK=\"\$SLURM_NODEID\"
    export NNODES=\"\$SLURM_NNODES\"
    export NPROC_PER_NODE=${GPUS_PER_NODE}

    # Forward HF cache/offline settings from host env
    export HF_HOME=${HF_HOME}
    export HF_HUB_CACHE=${HF_HUB_CACHE}
    export HUGGINGFACE_HUB_CACHE=${HUGGINGFACE_HUB_CACHE}
    export HF_DATASETS_CACHE=${HF_HOME}/datasets
    export HF_DATASETS_OFFLINE=1
    export TRANSFORMERS_OFFLINE=1
    export HF_HUB_OFFLINE=1
    export SSL_CERT_FILE=/etc/ssl/certs/ca-certificates.crt

    cd {{ repo_dir }}

    accelerate launch \
      --num_machines \$NNODES \
      --num_processes $WORLD_SIZE \
      --machine_rank \$NODE_RANK \
      --main_process_ip $MASTER_ADDR \
      --main_process_port $MASTER_PORT \
      --mixed_precision {{ mixed_precision }} \
      --dynamo_backend {{ dynamo_backend }} \
{% if use_deepspeed %}      --use_deepspeed \
{% endif %}      --deepspeed_multinode_launcher {{ deepspeed_multinode_launcher }} \
{% if same_network %}      --same_network \
{% endif %}      --rdzv_backend {{ rdzv_backend }} \
      --max_restarts 0 \
      --role \$(hostname -s): \
      scripts/train.py --config {{ config_path }}
  " &

wait $!
EXIT_CODE=$?

# ── Handle failure / resubmit ─────────────────────────────────────────────
if [ $EXIT_CODE -ne 0 ]; then
    FAILURES=$(cat "$FAILURE_COUNT_FILE")
    FAILURES=$((FAILURES + 1))
    echo $FAILURES > "$FAILURE_COUNT_FILE"
    if [ $FAILURES -lt $MAX_FAILURES ]; then
        echo "[$(date)] Failure $FAILURES/$MAX_FAILURES. Resubmitting..."
        sbatch "$0"
    else
        echo "[$(date)] Max failures reached ($MAX_FAILURES). Stopping."
        exit 1
    fi
fi
