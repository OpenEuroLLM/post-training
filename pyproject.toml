[project]
name = "post-training"
version = "0.1.0"
description = "A modular post-training framework for LLMs (SFT, DPO)"
readme = "README.md"
requires-python = ">=3.13,<3.15"
dependencies = [
    "torch",
    "trl",
    "deepspeed",
    "transformers",
    "datasets",
    "accelerate",
    "omegaconf",
    "wandb",
    "jinja2",
    "pyyaml",
    "kernels",
    "huggingface-hub",
    "flash_attn_3",
    "psutil",
    "xielu",
    "liger-kernel",
    "tensorboard>=2.20.0",
]

[project.optional-dependencies]
flash-attn-2 = ["flash-attn"]
dev = ["jupyter"]

[tool.uv.sources]
torch = { index = "pytorch-cuda" }
xielu = { git = "https://github.com/nickjbrowning/XIELU" }
flash-attn = [
    { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.8.3+cu130torch2.10-cp313-cp313-linux_aarch64.whl", marker = "platform_machine == 'aarch64' and python_version == '3.13'" },
    { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.8.3+cu130torch2.10-cp314-cp314-linux_aarch64.whl", marker = "platform_machine == 'aarch64' and python_version == '3.14'" },
    { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.8.3+cu130torch2.10-cp313-cp313-linux_x86_64.whl", marker = "platform_machine == 'x86_64' and python_version == '3.13'" },
    { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.8.3+cu130torch2.10-cp314-cp314-linux_x86_64.whl", marker = "platform_machine == 'x86_64' and python_version == '3.14'" },
]
flash_attn_3 = [
    { url = "https://github.com/windreamer/flash-attention3-wheels/releases/download/2026.02.12-d57f6d8/flash_attn_3-3.0.0+20260212.cu130torch2100cxx11abitrue.c4d8b0-cp39-abi3-linux_aarch64.whl", marker = "platform_machine == 'aarch64'" },
    { url = "https://github.com/windreamer/flash-attention3-wheels/releases/download/2026.02.12-d57f6d8/flash_attn_3-3.0.0+20260212.cu130torch2100cxx11abitrue.c4d8b0-cp39-abi3-linux_x86_64.whl", marker = "platform_machine == 'x86_64'" },
]

[[tool.uv.index]]
name = "pytorch-cuda"
url = "https://download.pytorch.org/whl/cu130"
explicit = true

[tool.uv]
override-dependencies = ["nvidia-cublas==13.0.2.14"]

[tool.uv.extra-build-dependencies]
xielu = ["cmake", "numpy", { requirement = "torch", match-runtime = true }]
